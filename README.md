
In this study, we perform a comprehensive evaluation of model performance, focusing on both accuracy and interpretability. We utilize LIME (Local Interpretable Model-agnostic Explanations) and SHAP (SHapley Additive exPlanations) to analyze feature importance across various machine learning models. By leveraging these interpretability techniques, we aim to identify which features have the most significant impact on model predictions. The combination of performance metrics and interpretability analysis provides a deeper understanding of model behavior, helping to determine which features contribute the most to improving model performance and overall decision-making.
![confusion](https://github.com/user-attachments/assets/1e881c2a-2855-43cd-b3d7-d3a6a39e4fe7)
![Roc](https://github.com/user-attachments/assets/7d0ec4c3-0961-463b-a107-0cdc47819a52)
![Screenshot 2024-08-29 233113](https://github.com/user-attachments/assets/dbe05193-d11e-460a-abf4-f6acd54bc742)
![Screenshot 2024-08-29 233133](https://github.com/user-attachments/assets/af082726-1126-4424-bd91-3205a738a900)
![Shap](https://github.com/user-attachments/assets/9e48138a-eaa6-45af-9a65-e729f32f7a46)




